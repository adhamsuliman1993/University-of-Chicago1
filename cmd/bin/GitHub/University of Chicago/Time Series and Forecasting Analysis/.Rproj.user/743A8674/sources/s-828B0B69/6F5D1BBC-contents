---
title: "Homework 5"
author: "Adham Suliman"
date: "August 8, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!"pacman" %in% rownames(installed.packages())) {
  install.packages("pacman")
}
pacman::p_load(tidyverse,faraway,corrplot, tinytex, dplyr,zoo, knitr, rgl, relaimpo, MuMIn, broom,ggplot2, tidyr, fields, cluster, data.table, reshape2,poLCA, stats, copula, caret, GGally, psych,pROC,RODBC, quantreg, ROCR, plot.new, rpart, rpart.plot, rattle, RColorBrewer, e1071,gains)
```


Clustreg Function
```{r, include=F}
clustreg=function(dat,k,tries,sed,niter){

set.seed(sed)
dat=as.data.frame(dat)
rsq=rep(NA,niter)
res=list()
rsq.best=0
    for(l in 1:tries) {

	c = sample(1:k,nrow(dat),replace=TRUE)
	yhat=rep(NA,nrow(dat))
	for(i in 1:niter) {		
		resid=pred=matrix(0,nrow(dat),k)
		for(j in 1:k){	
			pred[,j]=predict(glm(dat[c==j,],family="gaussian"),newdata=dat)		
			resid[,j] = (pred[,j]-dat[,1])^2
		}

	c = apply(resid,1,fun.index.rowmin)
	for(m in 1:nrow(dat)) {yhat[m]=pred[m,c[m]]}
	rsq[i] = cor(dat[,1],yhat)^2	
	#print(rsq[i])
	}
	
	if(rsq[niter] > rsq.best) {	
		rsq.best=rsq[niter]
		l.best=l
            c.best=c
		yhat.best=yhat
		}
    }

    for(i in k:1) res[[i]]=summary(lm(dat[c.best==i,]))
	
return(list(data=dat,nclust=k,tries=tries,seed=sed,rsq.best=rsq.best,number.loops=niter, Best.try=l.best,cluster=c.best,results=res))
}
fun.index.rowmin=function(x) {
    
    z=(1:length(x)) [x == min(x)]
    if(length(z) > 1) { z=sample(z,1)}
    return ( z ) }
```


```{r cars}
data("GermanCredit")
GermanCredit
GermanCredit.Ind <- (sample(1:(nrow(GermanCredit)),700,replace=F))
GermanCredit.Train <- GermanCredit[GermanCredit.Ind,]
GermanCredit.Test <- GermanCredit[-GermanCredit.Ind,]
GermanCredit.Train[,c(2,1,3,4,5,6,7)]
German.Train1 <- clustreg(GermanCredit.Train[,c(2,1,3,4,5,6,7)],1,6,1121,1)
German.Train2 <- clustreg(GermanCredit.Train[,c(2,1,3,4,5,6,7)],2,10,1121,100)
German.Train3 <- clustreg(GermanCredit.Train[,c(2,1,3,4,5,6,7)],3,10,1121,100)
```


```{r}
c("K = 1" = German.Train1$rsq.best,"K = 2" = German.Train2$rsq.best,"K = 3" =German.Train3$rsq.best)
k_clusters <- as.data.frame(cbind("Number_of_Clusters" =c(1,2,3),c(German.Train1$rsq.best,German.Train2$rsq.best,German.Train3$rsq.best))) 
class(k_clusters$Number_of_Clusters)

ggplot(k_clusters, aes(x=Number_of_Clusters,y=V2))+geom_line()+ylab("VAF")
```

```{r, include=F}
clustreg.predict=function(results,newdat){

	yhat=rep(NA,nrow(newdat))
	resid=pred=matrix(0,nrow(newdat),length(table(results$cluster)))
		
		for(j in 1:length(table(results$cluster))){			
			pred[,j]=predict(glm(results$data[results$cluster==j,],family="gaussian"),newdata=newdat)		
			resid[,j] = (pred[,j]-newdat[,1])^2
		}

	c = apply(resid,1,fun.index.rowmin)
	for(m in 1:nrow(newdat)) {yhat[m]=pred[m,c[m]]}
	rsq = cor(newdat[,1],yhat)^2	

return(list(results=results,newdata=newdat,cluster=c,yhat=yhat,rsq=rsq))

}

```

All cluster regressions seem to do a relatively good job of explaining the variance.
```{r}
predict.test1 <- clustreg.predict(German.Train1,GermanCredit.Test)
predict.test2 <- clustreg.predict(German.Train2,GermanCredit.Test)
predict.test3 <- clustreg.predict(German.Train3,GermanCredit.Test)
Test.DF <- as.data.frame(cbind("K"=c(1,2,3),"Rsq"=c(predict.test1$rsq,predict.test2$rsq,predict.test3$rsq)))
Test.DF
```

Below, Test data and train data are compared, and the regression with two clusters has the least difference of R2 between the Train and Test data sets. The two classes proves the least difference so that should be the number of classes used. 
```{r}
Test.df.1 <-c(German.Train1$rsq.best,German.Train2$rsq.best,German.Train3$rsq.best)
Train.df.1 <- c(predict.test1$rsq,predict.test2$rsq,predict.test3$rsq)
Final.df <- as.data.frame(cbind("Train"=c(German.Train1$rsq.best,German.Train2$rsq.best,German.Train3$rsq.best), "Test"=c(predict.test1$rsq,predict.test2$rsq,predict.test3$rsq),"Difference"=(Test.df.1-Train.df.1)/Test.df.1))
Final.df
```

