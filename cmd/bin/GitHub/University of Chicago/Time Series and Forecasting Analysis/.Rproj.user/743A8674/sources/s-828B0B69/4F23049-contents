---
title: "Linear & NonLinear Project"
author: "Adham Suliman"
date: "July 17, 2018"
output: html_document
---

```{r setup, include=FALSE}
#rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
options(width = 100, warn = 0)
if(!"pacman" %in% rownames(installed.packages())) {
  install.packages("pacman")
}
pacman::p_load(tidyverse,faraway,corrplot, tinytex, dplyr,zoo, knitr, rgl, relaimpo, MuMIn, broom,ggplot2, tidyr, fields, cluster, data.table, reshape2,poLCA, stats, AER, MASS, pscl, faraway, pscl, lattice, latticeExtra, copula,rmvdc, AER)
conditionalEcho<-F
```

1) & 2) & 3)
```{r}
Course.Project.Data <- read.csv('LinearNonLinear_MalfunctionData.csv', header = T)
Counting.Process<-as.data.frame(cbind(Time=Course.Project.Data$Time,Count=1:length(Course.Project.Data$Time)))
Counting.Process[1:20,]
max(Counting.Process$Time)
plot(Counting.Process$Time,Counting.Process$Count,type="s")
```
The counting process trajectory looks pretty smooth and grows steadily.

What does it tell you about the character of malfunctions and the reasons causing them?
This tells me that as time goes on, there seems to be a pretty constant count which could mean an average rate or "intensity" of the issue occuring over time. 

3.1)
```{r}
plot(Counting.Process$Time,Counting.Process$Count/Counting.Process$Time,type="l",ylab="Cumulative Intensity")
abline(h=Counting.Process$Count[length(Counting.Process$Count)]/
         Counting.Process$Time[length(Counting.Process$Time)])
abline(h=mean(Counting.Process$Count/Counting.Process$Time))
c(Last.Intensity=Counting.Process$Count[length(Counting.Process$Count)]/
         Counting.Process$Time[length(Counting.Process$Time)],
  Mean.Intensity=mean(Counting.Process$Count/Counting.Process$Time))
```
4)
```{r}
Course.Project.Data$Minute <- floor(Course.Project.Data$Time/60)
Course.Project.Data1 <-
  Course.Project.Data%>%
    dplyr::select(Minute, Temperature)%>%
    dplyr::group_by(Minute) %>%
    dplyr::summarise(Minute.counts = c(n()),Miunute.Temps = c(mean(Temperature)))
One.Minute.Counts.Temps<-   Course.Project.Data1%>%
                            dplyr::mutate(Minute.times=c((Minute*60)+30))%>%
                            dplyr::select(Minute.times, Minute.counts, Miunute.Temps)
NCourse.Project.Data <-vector()
for (x in 1:250){
  NCourse.Project.Data[x]<-(30+(60*(x-1)))
}
NCourse.Project.Data <- as.data.frame(matrix(NCourse.Project.Data,ncol=1),col.names="Minute.times")
colnames(NCourse.Project.Data)="Minute.times"
One.Minute.Counts.Temps <-
  NCourse.Project.Data %>%
  left_join(One.Minute.Counts.Temps,by="Minute.times")
One.Minute.Counts.Temps$Minute.counts[is.na(One.Minute.Counts.Temps$Minute.counts)] <- 0
plot(One.Minute.Counts.Temps$Minute.times,One.Minute.Counts.Temps$Minute.counts)
```
Poisson Data
```{r}
 Test.Deviance.Overdispersion.Poisson<-function(Sample.Size,Parameter.Lambda){
  my.Sample<-rpois(Sample.Size,Parameter.Lambda)
  Model<-glm(my.Sample~1,family=poisson)
  Dev<-Model$deviance
  Deg.Fred<-Model$df.residual
  (((Dev/Deg.Fred-1)/sqrt(2/Deg.Fred)>-1.96)&((Dev/Deg.Fred-1)/sqrt(2/Deg.Fred)<=1.96))*1
} 
Test.Deviance.Overdispersion.Poisson(100,1)
sum(replicate(300,Test.Deviance.Overdispersion.Poisson(100,1)))
exp(glm(rpois(1000,2)~1,family=poisson)$coeff)
```

Negative Binomial Data
```{r}
Test.Deviance.Overdispersion.NBinom<-function(Sample.Size,Parameter.prob){
  my.Sample<-rnbinom(Sample.Size,2,Parameter.prob)
  Model<-glm(my.Sample~1,family=poisson)
  Dev<-Model$deviance
  Deg.Fred<-Model$df.residual
  (((Dev/Deg.Fred-1)/sqrt(2/Deg.Fred)>-1.96)&((Dev/Deg.Fred-1)/sqrt(2/Deg.Fred)<=1.96))*1
} 
sum(replicate(300,Test.Deviance.Overdispersion.NBinom(100,.2)))

```

Apply the test to the one minute event counts
Do you see signs of over-dispersion?
Alpha is greater than 0 due to the signicant p-value below which shows over disperison. 
```{r}
GLM.model<-glm(One.Minute.Counts.Temps$Minute.counts~1,family=poisson)
GLM.model
(Disp.test <- dispersiontest(GLM.model))
```
Q: Does the test show overdispersion?
A: Yes it does because the alpha is above 0 and pour p-value is significant which shows over dispersion. 

4.1.3 Test against negative binomial distribution
Definitely not an NB model. 
```{r}
GLM.NM<-glm.nb(Minute.counts~1,One.Minute.Counts.Temps)
summary(GLM.NM)
odTest(GLM.NM)
```
Does this test show overdispersion?
Yes it does due to the significant P-value. 

5. Find the distribution of Poisson Intensity
5.1 Kolmlgrorov-Sminrnov Test
```{r}
sample1=rnorm(100)
sample2=rnorm(100,1,2)
Cum.Distr.Functions <- data.frame(sample1,sample2)
ecdfplot(~ sample1 + sample2, data=Cum.Distr.Functions, auto.key=list(space='right'))
```


```{r}
ks.test(sample1,sample2)
```
What does this output tell you about equivalence of the two distributions?
Because the p-value is significant, these samples have come from different populations.


```{r}
ks.test(sample1,"pnorm",mean=0,sd=1)
```
What does this output tell you?
Because the p-value is not significant, the 1st samples does come from a population with a normal distribution N(0,1). 



```{r}
ks.test(sample2,"pnorm",mean=0,sd=1)
```
Check equivalence of the empirical distribution of sample2 and theoretical distribution Norm(0,1).
The second sample comes from a population that doesn't have a normal distribution N(0,1).


Apply Kolmogorov-Smirnov test to Counting.Process$Time and theoretical exponential distribution with parameter equal to average intensity.
```{r}
Time.diff <- diff(Counting.Process$Time)
intensity <-1:length(Counting.Process$Time)/(Counting.Process$Time)
mean.intensity <- mean(intensity)
KS.Test.Event.Interval <- ks.test(Time.diff,"pexp",rate=mean.intensity)
c(KS.Test.Event.Interval$statistic,p.value=KS.Test.Event.Interval$p.value)
```

 theoretical exponential distribution with parameter equal to average intensity.
```{r}
Time.diff <- diff(Counting.Process$Time)
ks.test(Time.diff,"pexp",mean(Time.diff),sd(Time.diff))
```


```{r}
plot(ecdf(Time.diff))
```


```{r}
order(One.Minute.Counts.Temps$Minute.counts,decreasing=T)
Event.Intensities <- One.Minute.Counts.Temps$Minute.counts/60 
hist(Event.Intensities)
```
What distribution does this histogram remind you of?
This distributino reminds me of a poissoin distribution. 

```{r}
start <- seq(0,5,.2)
(Fitting.Normal <- fitdistr(Event.Intensities,densfun="normal"))
(Fitting.Exponential <- fitdistr(Event.Intensities, densfun="exponential"))
```

Logs
```{r}
Event.Intensities1 <- Event.Intensities + 10e-15
(Fitting.Logs<-fitdistr(Event.Intensities1,densfun="lognormal"))
Fitting.Logs
KS.LogNormal <- ks.test(Event.Intensities1,"plnorm",meanlog= -2.8474770 ,sd= 5.4111039)
KS.LogNormal
```





Beta
```{r}
mu <- mean(Event.Intensities)
var <- var(Event.Intensities)
estBetaParams <- function(mu, var) {
  alpha <- ((1 - mu) / var - 1 / mu) * mu ^ 2
  beta <- alpha * (1 / mu - 1)
  return(params = list(alpha = alpha, beta = beta))
}
x <- estBetaParams(mu,var)
x[2]
KS.beta <- ks.test(Event.Intensities,"pbeta", shape1=1.109756,shape2=4.340912)
```

Exponential distribution should be closer but it's not. 
```{r}
KS.Normal <- ks.test(Event.Intensities,"pnorm",mean=Fitting.Normal$estimate[1] ,sd= Fitting.Normal$estimate[2])
names(Fitting.Exponential)
KS.Exp <- ks.test(Event.Intensities,"pexp",rate=Fitting.Exponential$estimate[1])
c(KS.Normal$statistic,P.Value=KS.Normal$p.value)
c(KS.Exp$statistic,P.Value=KS.Exp$p.value)
```
What do you conclude from these tests?
These tests are telling that the data doesn't fit a normal distribution nor an exponential distribution because of the significant p values. 

Try to fit gamma distribution directly using fitdistr()

```{r}
#Fitting.Gamma <- fitdistr(Event.Intensities,densfun="gamma")
```
NANs are prouced, but why?
Since there is a negative value within the data, the gamma distribution cannot be utilized because it can take only positive values. 

Intensity is gamma with a normal distribution. 
```{r}
first.moment <- mean(Event.Intensities)
second.moment <- var(Event.Intensities)*(length(Event.Intensities))/(length(Event.Intensities)+1)
beta <- first.moment/second.moment
alpha <- first.moment*beta
(Moments.Rate <- beta)
(Moments.Shape <- alpha)
KS.Test.Moments.Gamma <- ks.test(Event.Intensities,"pgamma",rate=beta, shape=alpha)
KS.Test.Moments.Gamma
```

What distribution for the one-minute intensity of malfunctions do you choose?
In the end, I would choose the beta distribution due to it's high P value.

What distribution of one-minute malfunctions counts follow from your choice?
If I couldn't use the beta distribution, I would choose the gamma distribution. 
```{r}
rbind(KS.Moments=c(KS.Test.Moments.Gamma$statistic,P.Value=KS.Test.Moments.Gamma$p.value),
      KS.beta=c(KS.beta$statistic,P.Value=KS.beta$p.value),
      KS.LogNormal=c(KS.LogNormal$statistic,P.Value=KS.LogNormal$p.value),
      KS.Exp=c(KS.Exp$statistic,P.Value=KS.Exp$p.value),
      KS.Normal=c(KS.Normal$statistic,KS.Normal$p.value))
```


```{r}
write.csv(One.Minute.Counts.Temps,file="OneMinuteCountsTemps.csv",row.names=FALSE)
Part2.Data<-read.csv("OneMinuteCountsTemps.csv")
head(Part2.Data)
Part2.Data<-Part2.Data[complete.cases(Part2.Data),]
```


```{r}
Part2.Data<-as.data.frame(cbind(Part2.Data,Part2.Data[,2]/60))
colnames(Part2.Data)<-c("Times","Counts","Temperatures","Intensities")
head(Part2.Data)
plot(Part2.Data$Temperatures,Part2.Data$Intensities)
```
Interpret the plot. What type of relationship do you observe?
There initialy seems to be a high amount of data points in the temperatures between 95 to 105 where intensity is between 0 and .3.


```{r}
plot(rank(Part2.Data$Temperatures),rank(Part2.Data$Intensities))
```
What type of dependency you see in the empirical copula?
It seems as temperature increases, so does intensity but there data does seem to be quite sparsed. 

```{r}
hist(Part2.Data$Temperatures)
```


```{r}
head(Part2.Data)
Part2.Norm <- fitdistr(Part2.Data$Temperatures,"normal")
Part2.Norm$n
ks.test(Part2.Data$Temperatures,"pnorm",mean=100.069853 ,sd=4.812484  )
```



data intensities 
```{r}
Fitting.Copula <- cbind(pobs(Part2.Data$Temperatures, ties.method = "average"),pobs(Part2.Data$Intensities, ties.method = "average"))
Gumbelcopula<-gumbelCopula(dim=2)
Copula.Fit<-fitCopula(Gumbelcopula, 
                               pobs(Fitting.Copula,ties.method = "average"), 
                               method = "ml",
                               optim.method = "BFGS", 
                               optim.control = list(maxit=1000))

summary(Copula.Fit)
Copula.Fit@estimate
```

```{r}
plot(Part2.Data$Intensities)
```

https://stats.stackexchange.com/questions/90729/generating-values-from-copula-using-copula-package-in-r
```{r}

Fitting.Copula <- cbind(pobs(Part2.Data$Temperatures, ties.method = "average"),pobs(Part2.Data$Intensities, ties.method = "average"))
Gumbelcopula<-gumbelCopula(dim=2)
(Copula.Fit<-fitCopula(Gumbelcopula, 
                               pobs(Fitting.Copula,ties.method = "average"), 
                               method = "ml",
                               optim.method = "BFGS", 
                               optim.control = list(maxit=1000)))
summary(Copula.Fit)
```



```{r}
set.seed(8301735)
Simulated.Copula <- as.data.frame(rCopula(copula=gumbelCopula(Copula.Fit@estimate, dim=2), n=250))
Simulated.Copula
persp(gumbelCopula(Copula.Fit@estimate), dCopula, main="pdf",xlab="u", ylab="v", zlab="c(u,v)")
contour(gumbelCopula(Copula.Fit@estimate),dCopula, main="pdf",xlab="u", ylab="v")
plot(Simulated.Copula)
df <- data.frame(Temperatures = qnorm(Simulated.Copula[,1],mean=100.069853 ,sd=4.812484),
                 Intensities = qgamma(Simulated.Copula[,2],shape=1.655712, rate=8.132182/60 ))
Emperical.Copula <- as.data.frame(apply(df,2,rank))
plot(Emperical.Copula)
```


```{r}
set.seed(8301735)
Simulated.Copula1 <- as.data.frame(rCopula(copula=gumbelCopula(Copula.Fit@estimate,dim=2), n=5000))
df1 <- data.frame(Temperatures = qnorm(Simulated.Copula1[,1],mean=100.069853 ,sd=4.812484),
                 Intensities = qgamma(Simulated.Copula1[,2],shape=1.655712, rate=8.132182 ))
plot(df1)
Emperical.Copula1 <- as.data.frame(apply(df1,2,rank))
plot(Emperical.Copula1)
```

My numbers are slightly diferent than Yuri's below and I'm not exactly sure as to why. Have mercy! haaaha
```{r}
head(Part2.Data)
plot(Part2.Data$Temperatures,Part2.Data$Intensities)
NB.Fit.To.Sample <- glm.nb(Counts~Temperatures,Part2.Data)
NB.Fit.To.Sample$coefficients
NB.Fit.To.Sample$deviance
NB.Fit.To.Sample$df.residual
NB.Fit.To.Sample$aic
NB.Fit.To.Sample$theta
```


```{r}
Simulated.Temperature <- qnorm(Simulated.Copula1[,1],mean=100.069853 ,sd=4.812484)
Simulated.Intensities <- qgamma(Simulated.Copula1[,2],shape=1.655712, rate=8.132182 )
Simulated.Tails<-as.data.frame(
  cbind(round(Simulated.Intensities[(Simulated.Temperature>110)&(Simulated.Intensities>.5)]*60),
        Simulated.Temperature[(Simulated.Temperature>110)&(Simulated.Intensities>.5)]))
colnames(Simulated.Tails)<-c("Counts","Temperatures")
```


```{r}
plot(Simulated.Tails$Temperatures,Simulated.Tails$Counts)
```


With the Simulated tails, there is a lot less overdispersion which means data towards very high temperatures has a high correlation with the Intensity. 
```{r}
NB.Simulated.Tails <- glm.nb(Counts~Temperatures,Simulated.Tails)
NB.Simulated.Tails$theta
```
Is there an alternative model that you would try to fit to the simulated tail data?
I would also try fitting a poisson model which we will fit below 
What do both models tell you about the relationships between the temperature and the counts?
They don't fit well because of the high theta, but the models do a moderatley good job of fitting the data. 

```{r}
Poisson.Fit <- glm(Counts~Temperatures,Simulated.Tails,family="poisson")
Poisson.Fit$deviance
summary(Poisson.Fit)$df
Poisson.Fit$aic
dispersiontest(Poisson.Fit)
```




