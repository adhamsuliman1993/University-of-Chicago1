---
title: "Telco Customer Retention"
author: "Adham Suliman"
date: "August 21, 2018"
output: html_document
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
if(!"pacman" %in% rownames(installed.packages())) {
  install.packages("pacman")
}
pacman::p_load(tidyverse,faraway,corrplot, dplyr,zoo, knitr, rgl, relaimpo,ggplot2, tidyr, fields, cluster, data.table, reshape2,poLCA, stats,  caret, pROC,RODBC, quantreg, ROCR, rpart, rpart.plot, rattle, RColorBrewer, e1071,gains,nnet,factoextra, FactoMineR,MLmetrics,AER)

clustreg=function(dat,k,tries,sed,niter){

set.seed(sed)
dat=as.data.frame(dat)
rsq=rep(NA,niter)
res=list()
rsq.best=0
    for(l in 1:tries) {

	c = sample(1:k,nrow(dat),replace=TRUE)
	yhat=rep(NA,nrow(dat))
	for(i in 1:niter) {		
		resid=pred=matrix(0,nrow(dat),k)
		for(j in 1:k){	
			pred[,j]=predict(glm(dat[c==j,],family="gaussian"),newdata=dat)		
			resid[,j] = (pred[,j]-dat[,1])^2
		}

	c = apply(resid,1,fun.index.rowmin)
	for(m in 1:nrow(dat)) {yhat[m]=pred[m,c[m]]}
	rsq[i] = cor(dat[,1],yhat)^2	
	#print(rsq[i])
	}
	
	if(rsq[niter] > rsq.best) {	
		rsq.best=rsq[niter]
		l.best=l
            c.best=c
		yhat.best=yhat
		}
    }

    for(i in k:1) res[[i]]=summary(lm(dat[c.best==i,]))
	
return(list(data=dat,nclust=k,tries=tries,seed=sed,rsq.best=rsq.best,number.loops=niter, Best.try=l.best,cluster=c.best,results=res))
}
fun.index.rowmin=function(x) {
    
    z=(1:length(x)) [x == min(x)]
    if(length(z) > 1) { z=sample(z,1)}
    return ( z ) }
clustreg.predict=function(results,newdat){

	yhat=rep(NA,nrow(newdat))
	resid=pred=matrix(0,nrow(newdat),length(table(results$cluster)))
		
		for(j in 1:length(table(results$cluster))){			
			pred[,j]=predict(glm(results$data[results$cluster==j,],family="gaussian"),newdata=newdat)		
			resid[,j] = (pred[,j]-newdat[,1])^2
		}

	c = apply(resid,1,fun.index.rowmin)
	for(m in 1:nrow(newdat)) {yhat[m]=pred[m,c[m]]}
	rsq = cor(newdat[,1],yhat)^2	

return(list(results=results,newdata=newdat,cluster=c,yhat=yhat,rsq=rsq))

}
```



```{r cars}

data <- read.csv('Telco-Customer-Churn.csv')
str(data)

for (x in 1:nrow(data)){ 
   if (data[x,"PaymentMethod"] %in% c("Bank transfer (automatic)")){
     data[x,"PaymentMethod_code"] <- 1
   }else if(data[x,"PaymentMethod"] %in% c("Credit card (automatic)")){
     data[x,"PaymentMethod_code"]<-2
   }else if(data[x,"PaymentMethod"] %in% c("Credit card (automatic)")){
     data[x,"PaymentMethod_code"]<-3
   }else if (data[x,"PaymentMethod"] %in% c("Mailed check")){
     data[x,"PaymentMethod_code"]<-4
   }else{
     data[x,"PaymentMethod_code"]<-10
   }
}
unique(data$Contract)
for (x in 1:nrow(data)){ 
   if (data[x,"Contract"] %in% c("Month-to-month")){
     data[x,"Contract_code"] <- 1
   }else if(data[x,"Contract"] %in% c("One Year")){
     data[x,"Contract_code"]<-2
   }else if(data[x,"Contract"] %in% c("Two Year")){
     data[x,"Contract_code"]<-3
   }else{
     data[x,"Contract_code"]<-10
   }
}

for (x in 1:nrow(data)){ 
   if (data[x,"Churn"] %in% c("No")){
     data[x,"Churn_code"] <- 0
   }else if(data[x,"Churn"] %in% c("Yes")){
     data[x,"Churn_code"]<-1
    }else{
     data[x,"PaymentMethod_code"]<-10
   }
}
ind <- sample(1:nrow(data), 4930)
data.train <- data[ind,]
data.test <- data[-ind,]
```

Code manipulation for factors below:
factor analysis to understand wtf is going on with factors
```{r}
colnames(data)
#As tenure increased and total charges stayed low, 
ggplot(data,aes(x=tenure,y=MonthlyCharges, color=Churn))+geom_point()
data10 <- 
  data%>%
  group_by(tenure,Churn)%>%
  summarise(count=mean(MonthlyCharges))

data%>%
  group_by(tenure,Churn)%>%
  summarise(count=mean(MonthlyCharges))%>%
  ggplot(aes(x=tenure,y=count, color=Churn))+geom_point()
data10.n <- subset(data10,Churn=="No")
lm.n <- lm(count~tenure, data10.n)
summary(lm.n)
data10.y <- subset(data10,Churn=="Yes")
lm.y <- lm(count~tenure, data10.y)
summary(lm.y)
#People not happpy with fiber optics 
ggplot(data,aes(x=InternetService,y=MonthlyCharges, color=Churn))+geom_point()
```

```{r}
data6 <- data%>%
  dplyr::select(gender,SeniorCitizen,Partner,Dependents,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport, StreamingTV, StreamingMovies, Contract, PaperlessBilling,PaymentMethod)
data6[,2] <- as.factor(data6[,2])
x= apply(data6, 2, function(x) nlevels(as.factor(x)))
library("FactoMineR")
MCA1 <- MCA(data6)
MCA1$eig
#1-800 334 0110
```



```{r}
# data frame with variable coordinates
Mca1_vars_df = data.frame(MCA1$var$coord, Variable = rep(names(x), x))

# data fraMe with observation coordinates
Mca1_obs_df = data.frame(MCA1$ind$coord)
Mca1_vars_df
# plot of variable categories
ggplot(data=Mca1_vars_df, 
       aes(x = Dim.1, y = Dim.2, label = rownames(Mca1_vars_df))) +
 geom_hline(yintercept = 0, colour = "gray70") +
 geom_vline(xintercept = 0, colour = "gray70") +
 geom_text(aes(colour=Variable),check_overlap=T) +
 ggtitle("MCA plot of variables using R package FactoMineR")


# DeviceProtection
# InternetService
# OnlineSecurity
# OnlineBackup
# TechSupport
# StreamingTV
# StreamingMovies
```



Latent class analysis and choosing parameters from glm that were significant. STill need to go through this and figure out percentage of groups which belong to churn yes / churn no

Streaming TV and Streaming Movies look the same
```{r}
data3 <- data[,c('StreamingMovies', 'OnlineSecurity','DeviceProtection','InternetService', 'OnlineBackup', 'TechSupport')]
data.train3 <- data3[ind,]
data.test3 <- data3[-ind,]
table(data3$StreamingMovies)
f1 <- with(data.train, cbind(StreamingMovies, OnlineSecurity,DeviceProtection,InternetService, OnlineBackup, TechSupport)~1) 
results.4.train <- poLCA(f1,data.train3, nclass=3, nrep= 100, tol=.001, verbose=F, graphs=T)
results.4.test <- poLCA(f1,data.test3,probs.start = results.4.train$probs, nclass=3,  nrep= 100, tol=.001, verbose=F, graphs=T)
cbind(results.4.train,results.4.test)
names(results.4.test)
data.test$predclass <-results.4.test$predclass
results.4.train$Chisq
results.4.test$chisq
```
38% of Class 1 Churned. 
7% of Class 2 Churned.
27% of Class 3 Churned.


```{r}
data.train4 <- data.train[,c(19,6)]
data.test4 <- data.test[,c(19,6)]
train.kmeans.3<-kmeans(data.train4,centers=3,nstart=100)
train.kmeans.3.size<-train.kmeans.3$size/nrow(data.train4)
train.kmeans.3.centroids<-train.kmeans.3$centers
test.kmeans.3<-kmeans(data.test4,centers=3,nstart=100)
test.kmeans.3.size<-test.kmeans.3$size/nrow(data.test4)
test.kmeans.3.centroids<-test.kmeans.3$centers
```

```{r}
nrow(subset(data.test, Churn=="Yes" & predclass==1))/nrow(subset(data.test, predclass==1))
nrow(subset(data.test, Churn=="Yes" & predclass==2))/nrow(subset(data.test, predclass==2))
nrow(subset(data.test, Churn=="Yes" & predclass==3))/nrow(subset(data.test, predclass==3))
```


```{r}
train.kmeans.3<-kmeans(data.train4,centers=3,nstart=100)
train.kmeans.3.size<-train.kmeans.3$size/nrow(data.train4)
train.kmeans.3.centroids<-train.kmeans.3$centers
fviz_cluster(train.kmeans.3, data = data.train4)
test.kmeans.3<-kmeans(data.test4,centers=3,nstart=100)
test.kmeans.3.size<-test.kmeans.3$size/nrow(data.test4)
test.kmeans.3.centroids<-test.kmeans.3$centers
fviz_cluster(test.kmeans.3, data = data.test4)
cluster1<-as.data.frame(train.kmeans.3$cluster)
cluster2<-as.data.frame(test.kmeans.3$cluster)
cluster1$cluster<-cluster1$'train.kmeans.3$cluster'
cluster2$cluster<-cluster2$'test.kmeans.3$cluster'
cluster1$'train.kmeans.3$cluster'<-NULL
cluster2$'test.kmeans.3$cluster'<-NULL
clusters<-rbind(cluster1,cluster2)
```


```{r}
glm.train <- glm(Churn~gender+SeniorCitizen+Partner+Dependents+tenure+PhoneService+MultipleLines+InternetService+OnlineSecurity+OnlineBackup+DeviceProtection+TechSupport+StreamingTV+StreamingMovies+Contract+PaperlessBilling+PaymentMethod+MonthlyCharges+TotalCharges,data.train,family="binomial")
step(glm.train)
```


Using step function, we overfit the model with Residual deviance: 4089.0  on 4901  degrees of freedom.
Taking out predictors based off of least importance, a final model is created utilizing tenure and contract as predictors creating a model with a residual deviance o 4704.7 out of 4926 DOF, therefore we are slightly overfitting. 
A cutoff point o .743 was found to lead to the highest accuracy with a 74% Accuracy rate. Our model only predicted Non-Churners with 1572 from the training set correctly predicted as Churn No and 541 saying churn yes. 
```{r}
glm.train <- glm(Churn ~  tenure +  
     Contract , family = "binomial", data = data.train)
summary(glm.train)
pred <- predict(glm.train,newdata=data.test,type="response")
pred.1 <- as.vector(rep(0,2113))
pred.1 <- ifelse(pred>.743,1,0)
tab <- table(pred.1,data.test$Churn)
tab
sum(diag(tab))/sum(tab)
    
#1397/(268+1397)=.839
#1534/(1534+568)=.7297
```

Talk about gain and loss in type 1 and type 2 errors. 
```{r}
pred1 <- prediction(pred.1,data.test$Churn)
eval<- performance(pred1, "acc")
y.eval <- unlist(eval@y.values, use.names=F)
x.eval <- unlist(eval@x.values, use.names=F)
(y.eval.max <- max(y.eval))
new.eval <- as.data.frame(cbind(x.eval,y.eval))
new.eval <- subset(new.eval, x.eval <= 1)
eval.max <- as.data.frame(cbind(subset(new.eval,y.eval==y.eval.max)))
```



How to find the lowest point on tree
```{r pressure, echo=FALSE}
train.tree <- rpart(Churn ~ tenure +  
     Contract,data.train,method="class", control=rpart.control(cp=0,minsplit=10,xval=10, maxsurrogate=0))
(mytree.cp <- printcp(train.tree))
(mytree.plot <- plotcp(train.tree,minline=T))
```

77% Accuracy with tree model
```{r}
test.tree <- rpart(Churn ~ tenure +  
     Contract,data.test,method="class",control=rpart.control(cp=.0037,minsplit=10,xval=10, maxsurrogate=0))
printcp(test.tree)
tree.pred1 <- predict(test.tree, type="class")
length(data.test)
tab <- table(tree.pred1,data.test$Churn)
tab
sum(diag(tab))/sum(tab)
table(tree.pred1,data.test$Churn)
#1375+218)=.863
```


Do an analysis to see where revenue comes with and without children. 

```{r}
table(data$Churn)
data2 <- subset(data,Churn=="Yes")
data11 <- 
  data2%>%
  group_by(tenure)%>%
  summarise(count=n())
data2%>%
  group_by(tenure)%>%
  summarise(count=n())%>%
  ggplot(aes(tenure,count))+geom_bar(stat='identity')
head(data11)
glm.nb <- glm.nb(count~tenure,data11)
Poisson.Fit <- glm(count~tenure,data11,family="poisson")
dispersiontest(Poisson.Fit)
Poisson.Fit$coefficients
exp(Poisson.Fit$coefficients[2])
data2%>%
  summarise(count=mean(tenure))
data2.a <- data2%>%
  dplyr::select(gender,SeniorCitizen,Partner,Dependents,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport, StreamingTV, StreamingMovies, Contract, PaperlessBilling,PaymentMethod)
data2.a[,2] <- as.factor(data2.a[,2])
MCA2 <- MCA(data2.a)
```


```{r}
nrow(data2)*.7
ind2 <- sample(1:nrow(data2), 1308)
data.train2 <- data2[ind2,]
data.test2 <- data2[-ind2,]
table(data2$SeniorCitizen)

```


Look at the yes population for phone service 
```{r}
data%>%
  group_by(tenure,InternetService)%>%
  summarise(count=n())%>%
  ggplot(aes(x=tenure,count,color=InternetService))+geom_line()
plot(data$InternetService,data$Churn)
test <- multinom(MultipleLines~Churn,data)
z <- summary(test)$coefficients/summary(test)$standard.errors
p <- (1 - pnorm(abs(z), 0, 1))*2
p
```


```{r}
head(data.train)
eps=10e-15
data.train5 <- data.train[,c(23,24)]
data.test5 <- data.test[,c(23,24)]
data.train.multi <- multinom(Contract_code~Churn_code,data.train5)
(z <- summary(data.train.multi)$coefficients/summary(data.train.multi)$standard.errors)
(p <- (1 - pnorm(abs(z), 0, 1))*2)
predicted.prob.multinom.train <- predict(data.train.multi,newdata=data.test5,type="probs")
```


```{r}
data.train.multi <- multinom(PaymentMethod_code~SeniorCitizen,data.train)
predicted.prob.multinom.train <- predict(data.train.multi,type="probs")
log.loss.multinom.train <- MultiLogLoss(predicted.prob.multinom.train,data.train.multi$fitted.values)
log.loss.multinom.train
(z1 <- summary(data.train.multi)$coefficients/summary(data.train.multi)$standard.errors)
(p1 <- (1 - pnorm(abs(z1), 0, 1))*2)
```


```{r}
data.train.multi <- multinom(Contract_code~SeniorCitizen,data.train)
predicted.prob.multinom.train <- predict(data.train.multi,type="probs")
log.loss.multinom.train <- MultiLogLoss(predicted.prob.multinom.train,data.train.multi$fitted.values)
log.loss.multinom.train
summary(data.train.multi)$coefficients
(z1 <- summary(data.train.multi)$coefficients/summary(data.train.multi)$standard.errors)
(p1 <- (1 - pnorm(abs(z1), 0, 1))*2)
```





```{r}
data.train.multi <- multinom(PaymentMethod_code~SeniorCitizen,data.train)
predicted.prob.multinom.train <- predict(data.train.multi,type="probs")
log.loss.multinom.train <- MultiLogLoss(predicted.prob.multinom.train,data.train.multi$fitted.values)
log.loss.multinom.train
summary(data.train.multi)$coefficients
(z1 <- summary(data.train.multi)$coefficients/summary(data.train.multi)$standard.errors)
(p1 <- (1 - pnorm(abs(z1), 0, 1))*2)
```






```{r}
head(data.train)
data.train5 <- data.train[,c(22,23)]
data.test5 <- data.test[,c(22,23)]
data.train.multi <- multinom(Contract~Churn,data.train)
predicted.prob.multinom.train <- predict(data.train.multi,type="probs")
log.loss.multinom.train <- MultiLogLoss(predicted.prob.multinom.train,data.train.multi$fitted.values)
log.loss.multinom.train
```



```{r}
data.train.multi <- multinom(PaymentMethod_code~SeniorCitizen,data.train)
predicted.prob.multinom.train <- predict(data.train.multi,type="probs")
log.loss.multinom.train <- MultiLogLoss(predicted.prob.multinom.train,data.train.multi$fitted.values)
log.loss.multinom.train
```


